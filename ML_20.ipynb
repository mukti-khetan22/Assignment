{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16b6b8f",
   "metadata": {},
   "source": [
    "1.\tQ: What is the underlying concept of Support Vector Machines? A: The underlying concept of Support Vector Machines (SVMs) is to find the hyperplane that maximizes the margin between two classes of data points in a high-dimensional space.\n",
    "2.\tQ: What is the concept of a support vector? A: A support vector is a data point that is closest to the decision boundary (hyperplane) in an SVM model.\n",
    "3.\tQ: When using SVMs, why is it necessary to scale the inputs? A: It is necessary to scale the inputs in SVMs to ensure that all features contribute equally to the distance metrics and to avoid the domination of one feature over the others.\n",
    "4.\tQ: When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance? A: Yes, an SVM classifier can output a confidence score, which is the distance between the decision boundary and the data point. However, it does not provide a percentage chance.\n",
    "5.\tQ: Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem? A: It is generally recommended to use the dual form of the SVM problem for large datasets with many features.\n",
    "6.\tQ: Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C? A: To reduce underfitting in an SVM classifier with an RBF kernel, it is generally better to increase the value of gamma and/or the value of the regularization parameter C.\n",
    "7.\tQ: To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set? A: The QP parameters for the soft margin linear SVM classifier problem can be set using the data points and their corresponding labels.\n",
    "8.\tQ: On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours. A: The LinearSVC, SVC, and SGDClassifier models can be trained on a linearly separable dataset to see if they produce similar models.\n",
    "9.\tQ: On the MNIST dataset, train an SVM classifier. What level of precision can you achieve? A: On the MNIST dataset, an SVM classifier can achieve high levels of precision using one-versus-the-rest classification and tuning hyperparameters with small validation sets.\n",
    "10.\tQ: On the California housing dataset, train an SVM regressor.\n",
    "A: Support Vector Machines (SVM) is not commonly used for regression tasks because of its binary nature, and it's not straightforward to extend for multivariate regression. Other regression algorithms like Linear Regression, Decision Trees, or Random Forest are more commonly used. However, if one wants to use SVM, they can use a variant of SVM called Support Vector Regression (SVR) that works well with continuous output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1859c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
