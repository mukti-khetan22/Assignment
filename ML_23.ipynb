{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d844ba3",
   "metadata": {},
   "source": [
    "1.\tWhat are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?\n",
    "Ans: Key reasons for reducing the dimensionality of a dataset are to simplify the problem, reduce noise, and improve computational efficiency. The major disadvantages include loss of information and interpretability.\n",
    "2.\tWhat is the dimensionality curse?\n",
    "Ans: The dimensionality curse refers to the phenomenon where the number of features in a dataset grows exponentially, making it difficult to process and analyze the data.\n",
    "3.\tTell if it's possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?\n",
    "Ans: It is generally not possible to reverse the process of reducing the dimensionality of a dataset completely, as some information is inevitably lost. However, it may be possible to approximate the original dataset using the reduced features.\n",
    "4.\tCan PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?\n",
    "Ans: Yes, PCA can be used to reduce the dimensionality of a nonlinear dataset with a lot of variables, although it may not be as effective as other nonlinear dimensionality reduction techniques.\n",
    "5.\tAssume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?\n",
    "Ans: The number of dimensions in the resulting dataset would depend on the specific amount of variance explained by each principal component. However, a 95 percent explained variance ratio suggests that the resulting dataset would have significantly fewer dimensions than the original 1,000.\n",
    "6.\tWill you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?\n",
    "Ans: Vanilla PCA can be used for small to medium-sized datasets. Incremental PCA can be used for large datasets that do not fit in memory. Randomized PCA can be used for very large datasets where speed is important. Kernel PCA can be used for nonlinear datasets.\n",
    "7.\tHow do you assess a dimensionality reduction algorithm's success on your dataset?\n",
    "Ans: A dimensionality reduction algorithm's success can be assessed by measuring its ability to retain information from the original dataset, such as the amount of explained variance or the ability to reconstruct the original data. It can also be assessed by evaluating its impact on the performance of a downstream machine learning model.\n",
    "8.\tIs it logical to use two different dimensionality reduction algorithms in a chain?\n",
    "Ans: Yes, it is logical to use two different dimensionality reduction algorithms in a chain, as long as the second algorithm is able to capture information that was missed by the first algorithm. However, this may not always be necessary or beneficial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c0b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
