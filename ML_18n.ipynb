{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6075c0a7",
   "metadata": {},
   "source": [
    "\n",
    "What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point\n",
    "Answer:\n",
    "1.\tSupervised learning involves a labeled dataset with inputs and their corresponding outputs, and the algorithm learns to predict new outputs based on the input features. Examples include image classification and regression.\n",
    "2.\tUnsupervised learning involves an unlabeled dataset, and the algorithm learns to identify patterns or structures in the data. Examples include clustering and anomaly detection.\n",
    "\n",
    "Q2.\tMention a few unsupervised learning applications.\n",
    "Answer:\n",
    "1.\tClustering\n",
    "2.\tAnomaly detection\n",
    "3.\tDimensionality reduction\n",
    "4.\tAssociation rule mining\n",
    "5.\tTopic modeling\n",
    "Q3.\tWhat are the three main types of clustering methods? Briefly describe the characteristics of each.\n",
    "Answer:\n",
    "1.\tHierarchical clustering: This method creates a tree-like structure of nested clusters by successively merging smaller clusters into larger ones.\n",
    "2.\tPartitioning clustering: This method partitions the data into a fixed number of clusters, with each data point assigned to exactly one cluster.\n",
    "3.\tDensity-based clustering: This method groups together data points that are in high-density regions of the feature space and separates points in low-density regions.\n",
    "Q4.\tExplain how the k-means algorithm determines the consistency of clustering.\n",
    "Answer: The k-means algorithm aims to minimize the sum of squared errors (SSE) of each point to its nearest cluster center. The consistency of clustering is determined by the SSE, which is calculated as the sum of the squared distances between each point and its assigned cluster center. A lower SSE indicates a more consistent clustering, as the points are closer to their assigned centers.\n",
    "Q5.\tWith a simple illustration, explain the key difference between the k-means and k-medoids algorithms.\n",
    "Answer: K-means and k-medoids are both clustering algorithms, but k-medoids uses medoids (i.e., the most centrally located point in each cluster) as cluster centers, whereas k-means uses the mean of the data points in each cluster as the center. Therefore, k-medoids is more robust to outliers and noise than k-means.\n",
    "Q6.\tWhat is a dendrogram, and how does it work? Explain how to do it.\n",
    "Answer: A dendrogram is a tree-like diagram that represents the hierarchy of clusters in a hierarchical clustering algorithm. The diagram starts with each data point in its own cluster and successively merges the nearest clusters until there is only one cluster. The x-axis represents the data points, and the y-axis represents the distance between the clusters being merged.\n",
    "Q7.\tWhat exactly is SSE? What role does it play in the k-means algorithm?\n",
    "Answer: SSE stands for sum of squared errors and is a measure of how far each data point is from the center of its assigned cluster in the k-means algorithm. SSE is used to evaluate the quality of the clustering by minimizing the sum of squared distances between each point and its assigned cluster center. A lower SSE indicates a better clustering.\n",
    "Q8.\tWith a step-by-step algorithm, explain the k-means procedure.\n",
    "Answer:\n",
    "1.\tRandomly initialize k cluster centers.\n",
    "2.\tAssign each data point to the nearest cluster center.\n",
    "3.\tUpdate each cluster center to the mean of the data points assigned to it.\n",
    "4.\tRepeat steps 2-3 until convergence (i.e., when the cluster assignments no longer change).\n",
    "\n",
    "Q9.\tIn the sense of hierarchical clustering, define the terms single link and complete link.\n",
    "A: Single link clustering, also known as minimum distance clustering, connects the two nearest data points in a cluster. Complete link clustering, also known as maximum distance clustering, connects the two farthest data points in a cluster.\n",
    "Q10.\tHow does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point.\n",
    "A: The Apriori algorithm is a data mining technique that identifies the most frequent itemsets in a dataset, allowing businesses to identify patterns in customer purchasing behavior. By focusing only on frequently occurring itemsets, businesses can reduce the number of transactions that need to be examined, which lowers the measurement overhead. For instance, if an itemset containing apples and bananas is frequently bought together, it can be assumed that they are related, and businesses can utilize this information in their marketing strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
