{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7056c9c",
   "metadata": {},
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth. Answer: Feature engineering is the process of selecting and transforming raw data into meaningful features that can be used as input to machine learning models. It involves various aspects such as data cleaning, data transformation, feature selection, and feature extraction, which help to improve the performance of machine learning models by making them more accurate and efficient in their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e1b45",
   "metadata": {},
   "source": [
    "2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection? Answer: Feature selection is the process of selecting a subset of relevant features from a larger set of features to improve the performance of machine learning models. The aim of feature selection is to reduce the dimensionality of the data and remove irrelevant or redundant features, which can lead to overfitting and decreased accuracy of the model.\n",
    "\n",
    "There are several methods of feature selection, including:\n",
    "\n",
    "Filter methods: These methods rank the features based on statistical metrics like correlation, mutual information, and chi-square test, and select the top-ranked features.\n",
    "\n",
    "Wrapper methods: These methods use the machine learning algorithm itself to evaluate the performance of different feature subsets.\n",
    "\n",
    "Embedded methods: These methods incorporate feature selection into the model building process itself, for example, Lasso and Ridge regression.\n",
    "\n",
    "Dimensionality reduction methods: These methods reduce the number of features by transforming them into a lower-dimensional space, for example, Principal Component Analysis (PCA) and Singular Value Decomposition (SVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532bcd8d",
   "metadata": {},
   "source": [
    "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach? Answer: Filter approaches rank features based on statistical metrics and select the top-ranked features, while wrapper approaches use the machine learning algorithm itself to evaluate the performance of different feature subsets.\n",
    "\n",
    "The pros of filter approaches are that they are computationally efficient and can handle a large number of features, while the cons are that they may overlook important feature interactions and dependencies. The pros of wrapper approaches are that they consider feature interactions and dependencies, but the cons are that they can be computationally expensive and may overfit the model to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d21e4c",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
    "widely used function extraction algorithms? Answer:i. The overall feature selection process involves selecting a subset of relevant features from a larger set of features to improve the performance of machine learning models. This process typically involves data preprocessing, feature selection, and model building.\n",
    "\n",
    "ii. The key underlying principle of feature extraction is to transform the raw input data into a set of new features that capture the underlying patterns and relationships in the data. For example, in image processing, feature extraction algorithms can be used to extract edge or texture features from the image.\n",
    "\n",
    "Some widely used feature extraction algorithms include Principal Component Analysis (PCA) for linear transformations, and t-Distributed Stochastic Neighbor Embedding (t-SNE) for non-linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28138aa1",
   "metadata": {},
   "source": [
    "5. Describe the feature engineering process in the sense of a text categorization issue. Answer: The feature engineering process for text categorization involves preprocessing the text data, transforming it into a numerical representation, and selecting relevant features to train a machine learning model. This process can include steps such as tokenization, stop-word removal, stemming, and vectorization, as well as methods for feature selection such as mutual information or chi-square tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e88c96",
   "metadata": {},
   "source": [
    "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "cosine. Answer: Cosine similarity is a good metric for text categorization because it measures the similarity between two documents based on their word frequencies regardless of their length and scale.\n",
    "\n",
    "To find the cosine similarity between the two rows, we first calculate the dot product of the two vectors, which is (2x2 + 3x1 + 2x0 + 0x0 + 2x3 + 3x2 + 3x1 + 0x3 + 1x1) = 23. Then, we calculate the magnitude of each vector by taking the square root of the sum of squares of its values, which are sqrt(22) and sqrt(20) for the two rows. Finally, we divide the dot product by the product of the magnitudes to get the cosine similarity, which is 23 / (sqrt(22) * sqrt(20)) = 0.929."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe338a71",
   "metadata": {},
   "source": [
    "7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1). Answer:i. The formula for calculating Hamming distance is the number of positions where two binary strings differ.\n",
    "\n",
    "For the binary strings 10001011 and 11001111, the Hamming distance is 2.\n",
    "\n",
    "ii. The Jaccard index measures the similarity between two sets as the ratio of the intersection to the union of the sets, while the similarity matching coefficient measures the similarity as the ratio of the number of matching features to the total number of features.\n",
    "\n",
    "For the given feature vectors, the Jaccard index is 0.5 (the intersection is {1, 0, 0, 0, 1, 0, 0, 1}, and the union is {1, 1, 0, 0, 1, 0, 1, 1}), and the similarity matching coefficient is 0.625 (there are 5 matching features out of 8 total features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aea321",
   "metadata": {},
   "source": [
    "8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it? Answer:High-dimensional data sets refer to data sets with a large number of features or dimensions relative to the number of observations. Examples include text data with many words, images with high resolution, or genetic data with many genes.\n",
    "\n",
    "Difficulties in using machine learning techniques on high-dimensional data sets include the curse of dimensionality, overfitting, and increased computational complexity.\n",
    "\n",
    "To address these difficulties, dimensionality reduction techniques can be used to reduce the number of features, such as Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE), or regularization techniques can be used to control overfitting, such as Lasso or Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c462d",
   "metadata": {},
   "source": [
    "9. Make a few quick notes on:\n",
    "\n",
    "PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique\n",
    "Answer: PCA stands for Principal Component Analysis, a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining most of the variability in the original data.\n",
    "\n",
    "Vectors are mathematical objects that represent quantities with both magnitude and direction, and they are widely used in machine learning to represent data and models.\n",
    "\n",
    "Embedded techniques refer to machine learning algorithms that learn a lower-dimensional representation of the input data as part of the model training process, such as autoencoders or manifold learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b62f21",
   "metadata": {},
   "source": [
    "10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient Answer:Sequential backward exclusion and sequential forward selection are both feature selection methods, but the former removes features iteratively while the latter adds them.\n",
    "\n",
    "Filter and wrapper are both function selection methods, but the former applies a statistical measure to rank the features while the latter uses a machine learning model to evaluate the subset of features.\n",
    "\n",
    "SMC (Similarity Matching Coefficient) and Jaccard coefficient are both similarity measures, but the former computes the ratio of the matching features to the total number of features while the latter computes the ratio of the intersection to the union of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f06de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
