{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad18bf97",
   "metadata": {},
   "source": [
    "1.\tQ: What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set? A: It is difficult to estimate the depth of a Decision Tree trained on a one million instance training set without knowing the complexity of the problem and the number of features.\n",
    "2.\tQ: Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater? A: The Gini impurity of a node is usually lower than that of its parent because the split is made to reduce the impurity. However, it is not always lower as there can be cases where the split increases the impurity.\n",
    "3.\tQ: Explain if it's a good idea to reduce max depth if a Decision Tree is overfitting the training set? A: Yes, it is a good idea to reduce the maximum depth of a Decision Tree if it is overfitting the training set. By reducing the depth, the model becomes less complex and less likely to overfit.\n",
    "4.\tQ: Explain if it's a good idea to try scaling the input features if a Decision Tree underfits the training set? A: No, it is not a good idea to try scaling the input features if a Decision Tree underfits the training set. Decision Trees are not sensitive to the scale of the features, and scaling is unlikely to improve the performance of an underfitting model.\n",
    "5.\tQ: How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances? A: It is difficult to estimate the training time for a Decision Tree on a training set of 10 million instances without knowing the complexity of the problem and the number of features. However, it is likely to take much longer than an hour.\n",
    "6.\tQ: Will setting presort=True speed up training if your training set has 100,000 instances? A: No, setting presort=True is not likely to speed up training if the training set has 100,000 instances. Presorting is useful for small datasets, but it can slow down training for larger datasets.\n",
    "7.\tWhat are the steps to train and fine-tune a Decision Tree for the moons dataset?\n",
    "Answer: a) Build a moons dataset, b) Divide the dataset into training and test sets, c) Find good hyperparameters using grid search and cross-validation, d) Train the model on the entire training set and evaluate on the test set.\n",
    "\n",
    "8.\tWhat are the steps to grow a forest?\n",
    "Answer: a) Create 1,000 subsets of the training set, b) Train one Decision Tree on each subset using the best hyperparameters from the previous exercise, c) Create 1,000 Decision Tree predictions for each test set case and keep only the most common prediction, d) Evaluate these predictions on the test set to achieve a higher accuracy, indicating a successful Random Forest classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f63623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
